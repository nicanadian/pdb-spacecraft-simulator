"""ETE viewer validation tests - UI visualization tests.

Tests that simulation results display correctly in the viewer.

Key improvements over previous version:
- Uses REAL simulation output (not synthetic fixtures)
- Validates actual event counts match (not just >= 0)
- Checks CZML data matches simulation state
- Meaningful content validation (not just "page loaded")

Usage:
    pytest tests/ete/test_viewer_validation.py -v
    pytest tests/ete/ -m "ete_tier_a" -v
"""

from __future__ import annotations

import json
from datetime import datetime
from pathlib import Path
from typing import TYPE_CHECKING

import pytest

# Skip all tests if Playwright is not installed
try:
    from playwright.sync_api import expect

    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

if TYPE_CHECKING:
    from playwright.sync_api import Page


pytestmark = [
    pytest.mark.skipif(not PLAYWRIGHT_AVAILABLE, reason="Playwright not installed"),
    pytest.mark.ete_tier_a,
    pytest.mark.ete,
]


class TestRunLoading:
    """Test loading REAL simulation runs in the viewer."""

    def test_real_run_loads_in_viewer(self, viewer_page, completed_run):
        """
        Real simulation run loads in viewer without errors.

        This test uses actual simulation output (not synthetic data)
        to verify the viewer can load real results.
        """
        # Verify we're using real data
        assert not completed_run.is_synthetic(), (
            "Test must use real simulation output, not synthetic fixtures"
        )

        viewer_page.load_run(completed_run.path)

        assert viewer_page.is_loaded(), "Viewer failed to load run"
        assert not viewer_page.has_error(), (
            f"Viewer error: {viewer_page.get_error_message()}"
        )

    def test_manifest_data_matches_simulation(self, viewer_page, completed_run):
        """
        Verify viewer displays correct manifest data from simulation.

        Validates the viewer is parsing and displaying the actual
        simulation metadata, not hardcoded values.
        """
        viewer_page.load_run(completed_run.path)

        # The manifest from simulation should match what viewer loaded
        manifest = completed_run.manifest

        assert "plan_id" in manifest, "Simulation manifest missing plan_id"
        assert manifest["plan_id"], "Plan ID is empty"

        # Viewer should have loaded without errors
        assert viewer_page.is_loaded()


class TestEventDisplay:
    """Test event display matches simulation output."""

    def test_event_count_matches_simulation(self, viewer_page, completed_run):
        """
        Viewer displays the EXACT number of events from simulation.

        This is a key validation - the viewer must show all events,
        not more, not fewer.
        """
        viewer_page.load_run(completed_run.path)

        # Get event count from viewer
        viewer_events = viewer_page.get_alerts_count()

        # Get expected count from simulation output
        expected_events = completed_run.event_count

        # EXACT match required (not >= 0)
        assert viewer_events == expected_events, (
            f"EVENT COUNT MISMATCH\n"
            f"  Viewer shows:      {viewer_events} events\n"
            f"  Simulation output: {expected_events} events\n"
            f"\n"
            f"This indicates the viewer is not correctly loading events.json"
        )

    def test_constraint_violations_displayed(self, viewer_page, completed_run):
        """
        Constraint violations are correctly identified and displayed.

        Violations should be visually distinct from normal events.
        """
        viewer_page.load_run(completed_run.path)

        expected_violations = completed_run.constraint_violations

        # Get mission status which should include violation count
        status = viewer_page.get_mission_status()

        if "violations" in status:
            actual_violations = status["violations"]
            assert actual_violations == expected_violations, (
                f"VIOLATION COUNT MISMATCH\n"
                f"  Viewer shows:      {actual_violations} violations\n"
                f"  Simulation output: {expected_violations} violations"
            )


class TestCZMLValidation:
    """Test CZML trajectory data matches simulation output."""

    def test_czml_file_exists(self, completed_run):
        """
        Verify CZML file was generated by simulation.

        CZML is required for Cesium visualization.
        """
        czml_path = Path(completed_run.path) / "viz" / "scene.czml"

        assert czml_path.exists(), (
            f"CZML file not found at {czml_path}\n"
            "Simulation may not have generated visualization artifacts."
        )

        # Verify it's valid JSON
        with open(czml_path) as f:
            try:
                czml_data = json.load(f)
            except json.JSONDecodeError as e:
                pytest.fail(f"CZML file is not valid JSON: {e}")

        # CZML should have document header
        assert len(czml_data) > 0, "CZML file is empty"
        assert czml_data[0].get("id") == "document", (
            "CZML missing document header"
        )

    def test_czml_has_spacecraft_entity(self, completed_run):
        """
        Verify CZML contains spacecraft entity with position data.
        """
        czml_path = Path(completed_run.path) / "viz" / "scene.czml"

        if not czml_path.exists():
            pytest.skip("CZML file not generated")

        with open(czml_path) as f:
            czml_data = json.load(f)

        # Find spacecraft entity
        spacecraft = None
        for entity in czml_data:
            if entity.get("id") == "spacecraft" or "position" in entity:
                spacecraft = entity
                break

        assert spacecraft is not None, (
            "CZML missing spacecraft entity\n"
            f"Entities found: {[e.get('id') for e in czml_data]}"
        )

        assert "position" in spacecraft, (
            "Spacecraft entity missing position data"
        )

    def test_czml_position_matches_final_state(self, completed_run, physics_validator):
        """
        Verify CZML final position approximately matches simulation final state.

        This catches data corruption in the visualization export pipeline.
        """
        czml_path = Path(completed_run.path) / "viz" / "scene.czml"

        if not czml_path.exists():
            pytest.skip("CZML file not generated")

        if completed_run.final_state is None:
            pytest.skip("No final state available for comparison")

        with open(czml_path) as f:
            czml_data = json.load(f)

        # Find spacecraft with position
        spacecraft = None
        for entity in czml_data:
            if "position" in entity:
                spacecraft = entity
                break

        if spacecraft is None:
            pytest.skip("No spacecraft position in CZML")

        # Extract position from CZML (format varies)
        position_data = spacecraft["position"]

        # CZML can use different formats - handle the common ones
        if "cartographicDegrees" in position_data:
            # [lon, lat, alt] or [time, lon, lat, alt, ...]
            coords = position_data["cartographicDegrees"]
            # If time-tagged, get last position
            if len(coords) > 4:
                # Take last position (last 4 elements minus time)
                alt_km = coords[-1] / 1000.0  # Convert m to km
            else:
                alt_km = coords[3] / 1000.0 if len(coords) > 3 else coords[2] / 1000.0

            # Compare altitude (rough validation)
            final_pos = completed_run.final_state.position_eci
            import numpy as np
            final_alt_km = np.linalg.norm(final_pos) - 6378.137  # Earth radius

            # Allow 10km tolerance for CZML precision
            tolerance_km = 10.0
            assert abs(alt_km - final_alt_km) < tolerance_km, (
                f"CZML POSITION MISMATCH\n"
                f"  CZML altitude:  {alt_km:.1f} km\n"
                f"  Final state:    {final_alt_km:.1f} km\n"
                f"  Difference:     {abs(alt_km - final_alt_km):.1f} km\n"
                f"  Tolerance:      {tolerance_km:.1f} km\n"
                f"\n"
                f"This indicates the visualization export is corrupting data."
            )


class TestWorkspaces:
    """Test workspace functionality in viewer."""

    def test_default_workspace_is_mission_overview(self, viewer_page, completed_run):
        """Default workspace is mission overview."""
        viewer_page.load_run(completed_run.path)

        current_ws = viewer_page.current_workspace()
        assert current_ws == "mission-overview", (
            f"Expected default workspace 'mission-overview', got '{current_ws}'"
        )

    @pytest.mark.parametrize("workspace", [
        "maneuver-planning",
        "vleo-drag",
        "anomaly-response",
        "payload-ops",
    ])
    def test_workspace_switching(self, viewer_page, completed_run, workspace):
        """
        Each workspace can be accessed and displays without error.
        """
        viewer_page.load_run(completed_run.path)

        viewer_page.switch_workspace(workspace)

        assert viewer_page.current_workspace() == workspace, (
            f"Failed to switch to workspace '{workspace}'"
        )

        assert not viewer_page.has_error(), (
            f"Error in workspace '{workspace}': {viewer_page.get_error_message()}"
        )

    @pytest.mark.ete_tier_b
    def test_all_workspaces_cycle_without_error(self, viewer_page, completed_run):
        """
        All 5 workspaces accessible in sequence without cumulative errors.

        Tests for memory leaks or state corruption from rapid switching.
        """
        viewer_page.load_run(completed_run.path)

        workspaces = [
            "mission-overview",
            "maneuver-planning",
            "vleo-drag",
            "anomaly-response",
            "payload-ops",
        ]

        # Cycle through twice to catch cumulative issues
        for cycle in range(2):
            for ws in workspaces:
                viewer_page.switch_workspace(ws)
                assert viewer_page.current_workspace() == ws, (
                    f"Workspace switch failed on cycle {cycle + 1}"
                )
                assert not viewer_page.has_error(), (
                    f"Error after switching to {ws} on cycle {cycle + 1}"
                )


class TestTimeline:
    """Test timeline functionality in viewer."""

    def test_timeline_events_match_simulation(self, viewer_page, completed_run):
        """
        Timeline events should match simulation event output.
        """
        viewer_page.load_run(completed_run.path)

        timeline_events = viewer_page.get_timeline_events()

        # Timeline should have events if simulation produced any
        if completed_run.event_count > 0:
            assert len(timeline_events) > 0, (
                f"Timeline is empty but simulation has {completed_run.event_count} events"
            )

    @pytest.mark.ete_tier_b
    def test_timeline_scrubbing_updates_visualization(self, viewer_page, completed_run):
        """
        Timeline scrubbing should update the visualization.

        This is a visual/interaction test that verifies the timeline
        controls actually affect the display.
        """
        viewer_page.load_run(completed_run.path)

        # Wait for initial render
        viewer_page.page.wait_for_timeout(1000)

        # Scrub to middle
        viewer_page.scrub_to_time(0.5)

        # Should still be functional
        assert viewer_page.is_loaded()
        assert not viewer_page.has_error()

        # Scrub to end
        viewer_page.scrub_to_time(0.95)

        assert viewer_page.is_loaded()
        assert not viewer_page.has_error()


class TestErrorHandling:
    """Test graceful error handling in viewer."""

    def test_invalid_run_path_shows_error(self, viewer_page):
        """
        Invalid run path should show a clear error message.

        The app should NOT crash - it should display a user-friendly error.
        """
        viewer_page.load_run("/nonexistent/path/that/does/not/exist")

        # App should still be loaded (not crashed)
        assert viewer_page.is_loaded(), "Viewer crashed on invalid path"

        # Should have an error message OR gracefully show empty state
        # (depending on implementation)
        has_error = viewer_page.has_error()
        error_msg = viewer_page.get_error_message()

        if has_error:
            assert error_msg, "Error state but no error message"
            assert len(error_msg) > 10, "Error message too short to be useful"

    def test_missing_czml_handled_gracefully(self, viewer_page, tmp_path):
        """
        Missing CZML file should show error, not crash.
        """
        # Create incomplete run (manifest only, no CZML)
        viz_dir = tmp_path / "viz"
        viz_dir.mkdir()

        manifest = {
            "plan_id": "incomplete_run",
            "fidelity": "LOW",
            "_synthetic": True,
        }

        with open(viz_dir / "run_manifest.json", "w") as f:
            json.dump(manifest, f)

        viewer_page.load_run(str(tmp_path))

        # Should not crash
        assert viewer_page.is_loaded(), "Viewer crashed on missing CZML"


class TestDataIntegrity:
    """Test data integrity through the viewer pipeline."""

    def test_viewer_loads_real_events_file(self, completed_run):
        """
        Verify the events.json file exists and has expected structure.
        """
        events_path = Path(completed_run.path) / "viz" / "events.json"

        if not events_path.exists():
            # Events file may not exist if no events generated
            if completed_run.event_count == 0:
                return  # OK - no events, no file
            else:
                pytest.fail(
                    f"Expected {completed_run.event_count} events but "
                    f"events.json not found at {events_path}"
                )

        with open(events_path) as f:
            events = json.load(f)

        if isinstance(events, list):
            assert len(events) == completed_run.event_count, (
                f"Event count mismatch: file has {len(events)}, "
                f"expected {completed_run.event_count}"
            )

            # Each event should have required fields
            for i, event in enumerate(events):
                assert "id" in event or "type" in event, (
                    f"Event {i} missing id or type field"
                )
                assert "time" in event or "timestamp" in event, (
                    f"Event {i} missing time field"
                )

    def test_viewer_loads_real_manifest_file(self, completed_run):
        """
        Verify the run_manifest.json file exists and has expected structure.
        """
        manifest_path = Path(completed_run.path) / "viz" / "run_manifest.json"

        assert manifest_path.exists(), (
            f"Manifest file not found at {manifest_path}\n"
            "Simulation should always produce a manifest."
        )

        with open(manifest_path) as f:
            manifest = json.load(f)

        # Required fields
        required_fields = ["plan_id"]
        for field in required_fields:
            assert field in manifest, f"Manifest missing required field: {field}"


@pytest.mark.ete_tier_b
class TestPerformance:
    """Performance and responsiveness tests."""

    def test_workspace_switch_responsive(self, viewer_page, completed_run):
        """Workspace switching should complete within 2 seconds."""
        viewer_page.load_run(completed_run.path)

        start = datetime.now()
        viewer_page.switch_workspace("maneuver-planning")
        elapsed = (datetime.now() - start).total_seconds()

        assert elapsed < 2.0, (
            f"Workspace switch took {elapsed:.2f}s, expected < 2.0s"
        )

    def test_no_js_errors_during_interaction(self, page, viewer_page, completed_run):
        """
        No JavaScript errors during normal interaction.

        Captures console errors during workspace switching and
        timeline scrubbing to catch runtime bugs.
        """
        errors = []

        def handle_console(msg):
            if msg.type == "error":
                errors.append(msg.text)

        page.on("console", handle_console)

        viewer_page.load_run(completed_run.path)

        # Interact with viewer
        for ws in ["maneuver-planning", "vleo-drag", "mission-overview"]:
            viewer_page.switch_workspace(ws)
            viewer_page.page.wait_for_timeout(200)

        # Filter critical errors only
        critical_errors = [
            e for e in errors
            if any(err in e for err in ["TypeError", "ReferenceError", "SyntaxError"])
        ]

        assert len(critical_errors) == 0, (
            f"JavaScript errors during interaction:\n"
            + "\n".join(f"  - {e}" for e in critical_errors)
        )
